{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tehnike i modeli analize sentimenta u NLP-u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Priprema projekta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.venv/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: autocorrect in ./.venv/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: gensim in ./.venv/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./.venv/lib/python3.11/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in ./.venv/lib/python3.11/site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.venv/lib/python3.11/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (1.24.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install autocorrect\n",
    "!pip install matplotlib\n",
    "!pip install gensim\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priprema foldera\n",
    "!mkdir -p inputs\n",
    "!mkdir -p outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Učitavanje podataka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Kao izvor podataka, koristio sam <a href=\"http://help.sentiment140.com/for-students\">Sentiment-140</a> dataset za analizu sentimenta.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = [\n",
    "    'polarity',\n",
    "    'id',\n",
    "    'post_datetime',\n",
    "    'query',\n",
    "    'user',\n",
    "    'tweet'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'inputs/sentiment-140-dataset.csv', \n",
    "    encoding='UTF', \n",
    "    names=colNames,\n",
    "    encoding_errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Možemo iskoristiti random biblioteku za učitavanje nasumičnog subseta podataka. Ja ću stalno koristiti isti seed tako da svaki put dobijem isti subset podataka.</p>\n",
    "<p>Uzeli smo subset od 20000 podataka zato da možemo relativno brzo istrenirati model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# uzmi random subset\n",
    "# ukoliko zelis da ti svaki put subset bude isti, uzmi isti seed\n",
    "df_subset = df[['polarity', 'tweet']].sample(20000, random_state=46)\n",
    "# spremanje naseg subseta u csv\n",
    "df_subset.to_csv('outputs/sentiment-140-subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = pd.read_csv('outputs/sentiment-140-subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity\n",
       "0    10009\n",
       "4     9991\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Možemo primjetiti da naš dataset nema drugih polariteta osim pozitivnog i negativnog (nema neutralnog polariteta). To znači da će nam biti jednostavnije trenirati naš model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.polarity.replace(4, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Većina binarnih algoritama očekuje 0 ili 1 kao klasifikaciju. Zato ćemo, radi jednostavnosti, sve četvorke zamijeniti s jedinicama.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesiranje teksta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ciscenje teksta\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tomislav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tomislav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formalize(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def removeMentionsAndHashTags(text):\n",
    "    hashtagPattern = re.compile(r\"#[A-Za-z0-9_]+\")\n",
    "    mentionPattern = re.compile(r\"@[A-Za-z0-9_]+\")\n",
    "    \n",
    "    #Removing hashtags and mentions\n",
    "    text = hashtagPattern.sub(\"\", text)\n",
    "    text = mentionPattern.sub(\"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def removeUrls(text):\n",
    "    urlPatterns = [\n",
    "        re.compile(r\"https?://\\S+\"),\n",
    "        re.compile(r\"www\\.\\S+\")\n",
    "    ]\n",
    "    #Removing links\n",
    "    for pattern in urlPatterns:\n",
    "        text = pattern.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "def removeNumbers(text):\n",
    "    #removing numbers\n",
    "    text = re.sub(\"[0-9]\",\"\", text)\n",
    "    return text\n",
    "\n",
    "# mozda maknuti\n",
    "def removePunctuation(text):\n",
    "    #removing punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    return text\n",
    "\n",
    "def removeSingleQuotes(text):\n",
    "    #removing single quotes\n",
    "    text = re.sub(r\"\\'\",\"\", text)\n",
    "    return text\n",
    "\n",
    "def removeStopwords(text):\n",
    "    #removing stopwords\n",
    "    text = text.split(' ')\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def preprocess(\n",
    "    text,\n",
    "    HASH_TAGS_MENTIONS=False,\n",
    "    URLS=False,\n",
    "    NUMBERS=False,\n",
    "    PUNCTUATION=False,\n",
    "    SINGLE_QUOTES=False,\n",
    "    STOP_WORDS=True,\n",
    "    SHORT_WORDS=False\n",
    "):\n",
    "    spell = Speller(lang='en')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = \" \".join([word.lower() for word in text.split()])\n",
    "\n",
    "    if(not HASH_TAGS_MENTIONS):\n",
    "        text = removeMentionsAndHashTags(text)\n",
    "    if(not URLS):\n",
    "        text = removeUrls(text)\n",
    "    if(not NUMBERS):\n",
    "        text = removeNumbers(text)\n",
    "    if(not STOP_WORDS):\n",
    "        text = removeStopwords(text)\n",
    "    if(not PUNCTUATION):\n",
    "        text = removePunctuation(text)  \n",
    "    if(not SINGLE_QUOTES):\n",
    "        text = removeSingleQuotes(text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [formalize(token) for token in tokens]\n",
    "    tokens = [spell(token) for token in tokens]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if len(token) > 1]\n",
    "\n",
    "    # remove short words\n",
    "    if(not SHORT_WORDS):\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.tweet.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [preprocess(tweet) for tweet in df_subset.tweet.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# podjela na train i test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tweets,\n",
    "    df_subset.polarity.values,\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes klasifikatori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes klasifikator koristeći one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(X,y):\n",
    "    #build the dataset\n",
    "    words = [word_tokenize(sent) for sent in X]\n",
    "    dataset = list(zip(words, y))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "all_words = FreqDist(sum([w.split(\" \") for w in X_train],[]))\n",
    "word_features = list(all_words)[:2000]\n",
    "def documentFeatures(words):\n",
    "    wordSet = set(words)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in wordSet)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/bernoulli_array.pickle', 'wb') as f:\n",
    "    pickle.dump(word_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(documentFeatures(d), y) for (d,y) in buildDataset(X_train, y_train)]\n",
    "test_set = [(documentFeatures(d), y) for (d,y) in buildDataset(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def saveClassifier(classifier, filePath):\n",
    "   f = open(filePath, 'wb')\n",
    "   pickle.dump(classifier, f, -1)\n",
    "   f.close()\n",
    "\n",
    "def loadClassifier(filePath):\n",
    "   f = open(filePath, 'rb')\n",
    "   classifier = pickle.load(f)\n",
    "   f.close()\n",
    "   return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75      2083\n",
      "           1       0.73      0.76      0.74      1917\n",
      "\n",
      "    accuracy                           0.75      4000\n",
      "   macro avg       0.75      0.75      0.75      4000\n",
      "weighted avg       0.75      0.75      0.75      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preciznost modela:\n",
    "y_pred = [nb_classifier.classify(d) for (d, _) in test_set]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(arch) = True                0 : 1      =     15.3 : 1.0\n",
      "           contains(sad) = True                0 : 1      =     14.9 : 1.0\n",
      "           contains(rip) = True                0 : 1      =     13.2 : 1.0\n",
      "         contains(smile) = True                1 : 0      =     13.0 : 1.0\n",
      "         contains(proud) = True                1 : 0      =     12.1 : 1.0\n",
      "         contains(sadly) = True                0 : 1      =     12.0 : 1.0\n",
      "          contains(sick) = True                0 : 1      =     11.6 : 1.0\n",
      "         contains(bless) = True                1 : 0      =     11.5 : 1.0\n",
      "       contains(welcome) = True                1 : 0      =     11.5 : 1.0\n",
      "           contains(boo) = True                0 : 1      =     11.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveClassifier(nb_classifier, 'outputs/bernoulli_naive_bayes.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zanimljiv rezultat se događa ukoliko pokušamo maknuti stopwords iz modela**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_stops = [preprocess(tweet, STOP_WORDS=True) for tweet in X_train]\n",
    "X_test_no_stops = [preprocess(tweet, STOP_WORDS=True) for tweet in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(documentFeatures(d), y) for (d,y) in buildDataset(X_train_no_stops, y_train)]\n",
    "test_set = [(documentFeatures(d), y) for (d,y) in buildDataset(X_test_no_stops, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_no_stops_classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>U nekim slučajevima, stopwords poboljšavaju sveukupnu točnost modela, a nekad poboljšavaju prosječnu točnost modela, ali pogoršavaju točnost predviđanja pozitivnih rezultata, ovisno o datasetu.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75      2083\n",
      "           1       0.73      0.76      0.74      1917\n",
      "\n",
      "    accuracy                           0.75      4000\n",
      "   macro avg       0.75      0.75      0.75      4000\n",
      "weighted avg       0.75      0.75      0.75      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [nb_no_stops_classifier.classify(d) for (d, _) in test_set]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveClassifier(nb_no_stops_classifier, 'outputs/bernoulli_naive_bayes_no_stops.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes klasifikator koristeći one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "X_train_gaussian = vectorizer.fit_transform(X_train)\n",
    "X_test_gaussian = vectorizer.transform(X_test)\n",
    "\n",
    "vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('outputs/count_vectorizer.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.45      0.56      2033\n",
      "           1       0.59      0.82      0.69      1967\n",
      "\n",
      "    accuracy                           0.63      4000\n",
      "   macro avg       0.66      0.64      0.62      4000\n",
      "weighted avg       0.66      0.63      0.62      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# treniranje modela\n",
    "gaussian_nb_classifier = GaussianNB()\n",
    "gaussian_nb_classifier.fit(X_train_gaussian.toarray(), y_train)\n",
    "y_pred = gaussian_nb_classifier.predict(X_test_gaussian.toarray())\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print a classification report with precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveClassifier(gaussian_nb_classifier, 'outputs/gaussian_naive_bayes.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes klasifikator koristeći TF-IDF vektorizaciju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76      2083\n",
      "           1       0.73      0.75      0.74      1917\n",
      "\n",
      "    accuracy                           0.75      4000\n",
      "   macro avg       0.75      0.75      0.75      4000\n",
      "weighted avg       0.75      0.75      0.75      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multi_nb_classifier = MultinomialNB()\n",
    "multi_nb_classifier.fit(X_train_tfidf.toarray(), y_train)\n",
    "y_pred = multi_nb_classifier.predict(X_test_tfidf.toarray())\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print a classification report with precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveClassifier(multi_nb_classifier, 'outputs/multinomial_naive_bayes.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('outputs/tfidf_vectorizer.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistička regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistička regresija koristeći word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priprema word2vec modela\n",
    "vectorSize = 100\n",
    "word2VecModel = Word2Vec(\n",
    "    sentences = [[word.lower() for word in sent] for sent in brown.sents()], \n",
    "    vector_size = vectorSize, \n",
    "    window = 5, \n",
    "    min_count = 2, \n",
    "    sg = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Kreiranje word embeddinga (korištenjem average word vectors tehnike)\n",
    "def averageWordVectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    n_words = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            n_words += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "\n",
    "    if n_words > 0:\n",
    "        feature_vector = np.divide(feature_vector, n_words)\n",
    "\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = [averageWordVectors(doc, word2VecModel, word2VecModel.wv.index_to_key, vectorSize) for doc in [word_tokenize(sent) for sent in X_train]]\n",
    "X_test_w2v = [averageWordVectors(doc, word2VecModel, word2VecModel.wv.index_to_key, vectorSize) for doc in [word_tokenize(sent) for sent in X_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.62      0.63      2083\n",
      "           1       0.61      0.63      0.62      1917\n",
      "\n",
      "    accuracy                           0.63      4000\n",
      "   macro avg       0.63      0.63      0.63      4000\n",
      "weighted avg       0.63      0.63      0.63      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomislav/Desktop/rj/projekt/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Treniranje modela logističke regresije\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_w2v, y_train)\n",
    "y_pred = lr_model.predict(X_test_w2v)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveClassifier(lr_model, 'outputs/logistic_regression.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LSTM - Long Short Term Memory Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Embedding,Dense,Dropout,Bidirectional,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None) #num_words označava maksimalan broj riječi koje će se uzeti u obzir\n",
    "#num_words=None znači da će se uzeti sve riječi\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(tokenizer.word_counts.items())\n",
    "num_of_tokens = dict_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pripremljene podatke pretvaramo u nizove\n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "#Spremamo najdulju duljinu nekog tweeta\n",
    "max_tweet_length = np.max(np.array([len(t) for t in X_seq]))\n",
    "\n",
    "X_train_padded = pad_sequences(X_seq, maxlen=max_tweet_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(file):\n",
    "    embedding_model = {}\n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            embedding_model[word] = embedding\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index_fasttext = load_embedding_model('inputs/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix(num_tokens,embedding_dim,embedding_index):\n",
    "    hits=0\n",
    "    misses=[]\n",
    "\n",
    "    # Pripremamo matricu za embedding\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Rijeci koje nisu pronađene u embedding modelu će biti sve nule\n",
    "            # Ovo uključuje reprezentaciju za \"padding\" i \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses.append(word)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "embedding_matrix_fasttext = embedding_matrix(num_of_tokens,embedding_dim,embedding_index_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_fasttext = Embedding(\n",
    "    input_dim=num_of_tokens,\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=max_tweet_length,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix_fasttext),\n",
    "    trainable=False,\n",
    ")\n",
    "lstm_model = Sequential()\n",
    "#enbedding\n",
    "lstm_model.add(embedding_layer_fasttext)\n",
    "lstm_model.add(Dropout(0.5))\n",
    "#LSTM\n",
    "lstm_model.add(Bidirectional(LSTM(8,dropout=0.5,recurrent_dropout=0.2)))\n",
    "# Vannila skriveni sloj\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dense(16, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(units=1, activation='sigmoid',name='predictions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-3, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 - 8s - loss: 0.6529 - accuracy: 0.6122 - val_loss: 0.6022 - val_accuracy: 0.6672 - 8s/epoch - 38ms/step\n",
      "Epoch 2/10\n",
      "200/200 - 6s - loss: 0.6106 - accuracy: 0.6745 - val_loss: 0.5496 - val_accuracy: 0.7275 - 6s/epoch - 32ms/step\n",
      "Epoch 3/10\n",
      "200/200 - 6s - loss: 0.5968 - accuracy: 0.6914 - val_loss: 0.5281 - val_accuracy: 0.7469 - 6s/epoch - 32ms/step\n",
      "Epoch 4/10\n",
      "200/200 - 6s - loss: 0.5889 - accuracy: 0.6973 - val_loss: 0.5183 - val_accuracy: 0.7428 - 6s/epoch - 32ms/step\n",
      "Epoch 5/10\n",
      "200/200 - 6s - loss: 0.5834 - accuracy: 0.7009 - val_loss: 0.5097 - val_accuracy: 0.7491 - 6s/epoch - 32ms/step\n",
      "Epoch 6/10\n",
      "200/200 - 7s - loss: 0.5713 - accuracy: 0.7102 - val_loss: 0.5202 - val_accuracy: 0.7447 - 7s/epoch - 33ms/step\n",
      "Epoch 7/10\n",
      "200/200 - 7s - loss: 0.5733 - accuracy: 0.7122 - val_loss: 0.5130 - val_accuracy: 0.7500 - 7s/epoch - 33ms/step\n",
      "Epoch 8/10\n",
      "200/200 - 7s - loss: 0.5687 - accuracy: 0.7116 - val_loss: 0.5192 - val_accuracy: 0.7475 - 7s/epoch - 35ms/step\n",
      "Epoch 9/10\n",
      "200/200 - 7s - loss: 0.5635 - accuracy: 0.7120 - val_loss: 0.5122 - val_accuracy: 0.7547 - 7s/epoch - 34ms/step\n",
      "Epoch 10/10\n",
      "200/200 - 7s - loss: 0.5589 - accuracy: 0.7245 - val_loss: 0.4993 - val_accuracy: 0.7522 - 7s/epoch - 34ms/step\n"
     ]
    }
   ],
   "source": [
    "history = lstm_model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model evaluation with fasttext 300d embedding:\n",
      "125/125 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7590\n",
      "[0.5128079652786255, 0.7590000033378601]\n"
     ]
    }
   ],
   "source": [
    "Qtest = tokenizer.texts_to_sequences(X_test)\n",
    "Ptest = pad_sequences(Qtest,maxlen=max_tweet_length,padding='post' )\n",
    "print(\"LSTM model evaluation with fasttext 300d embedding:\")\n",
    "print(lstm_model.evaluate(Ptest, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomislav/Desktop/rj/projekt/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "lstm_model.save('outputs/lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Google BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomislav/Desktop/rj/projekt/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textCleaner(text):\n",
    "    text = text.lower()\n",
    "    text = removeMentionsAndHashTags(text)\n",
    "    text = removeUrls(text)\n",
    "    text = removeNumbers(text)\n",
    "    return text\n",
    "\n",
    "def preprocess(X):\n",
    "    X_cleaned = [textCleaner(text) for text in X]\n",
    "    return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exampleConverter(text, tokenizer, max_length=128):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,  \n",
    "        max_length=max_length, \n",
    "        pad_to_max_length=True,  \n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return encoding\n",
    "\n",
    "def exampleToDict(input_ids, attention_mask, token_type_ids, label):\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeExamples(X, y, tokenizer, max_length=128):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for text, label in zip(X, y):\n",
    "        bert_input = exampleConverter(text, tokenizer, max_length)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "        \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list))\n",
    "    return dataset.map(exampleToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podaci za treniranje\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "ds_train_encoded = encodeExamples(preprocess(X_train), y_train, bert_tokenizer).shuffle(100).batch(32).repeat(2)\n",
    "ds_val_encoded = encodeExamples(preprocess(X_validation), y_validation, bert_tokenizer).batch(32)\n",
    "ds_test_encoded = encodeExamples(preprocess(X_test), y_test, bert_tokenizer).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-5\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 8798s 11s/step - loss: 0.4229 - accuracy: 0.8073 - val_loss: 0.4779 - val_accuracy: 0.7891\n",
      "Epoch 2/2\n",
      "800/800 [==============================] - 9003s 11s/step - loss: 0.1553 - accuracy: 0.9421 - val_loss: 0.7301 - val_accuracy: 0.7663\n",
      "accuracy: 74.70%\n"
     ]
    }
   ],
   "source": [
    "bert_model.fit(ds_train_encoded, epochs=2, validation_data=ds_val_encoded)\n",
    "\n",
    "loss, acc = bert_model.evaluate(ds_test_encoded, verbose=0)\n",
    "print(\"accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs/bert_model/saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs/bert_model/saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "bert_model.save_pretrained(\"outputs/bert_model\", saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outputs/bert_tokenizer/tokenizer_config.json',\n",
       " 'outputs/bert_tokenizer/special_tokens_map.json',\n",
       " 'outputs/bert_tokenizer/vocab.txt',\n",
       " 'outputs/bert_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.save_pretrained(\"outputs/bert_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
